{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up Environment**"
      ],
      "metadata": {
        "id": "Py4LfiPSN61H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2I5LQbzNwB9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tic-Tac-Toe Game**"
      ],
      "metadata": {
        "id": "h04IXLDFOAi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "  def __init__(self, width, height, state=None):\n",
        "    self.board = np.array(state, dtype=int) if state is not None else np.zeros((height, width), dtype=int)\n",
        "    self.players = [\"X\", \"O\"]\n",
        "    self.current_player = self.players[0]\n",
        "    self.winner = None\n",
        "    self.game_over = False\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "\n",
        "  # Reset the Game\n",
        "  def reset(self):\n",
        "    self.board = np.zeros((self.height, self.width), dtype=np.int16)\n",
        "    self.current_player = self.players[0]\n",
        "    self.winner = None\n",
        "    self.game_over = False\n",
        "    return self.board.copy()\n",
        "\n",
        "  # List of Available Moves\n",
        "  def available_moves(self):\n",
        "    moves = []\n",
        "    for i in range(self.height):\n",
        "      for j in range(self.width):\n",
        "        if self.board[i][j] == 0:\n",
        "          moves.append((i, j))\n",
        "    return moves\n",
        "\n",
        "  # Make a Move & Getting Reward/Penalty if there is a Winner\n",
        "  def make_move(self, move):\n",
        "    if self.board[move[0], move[1]] != 0 or self.game_over:\n",
        "      return self.board.copy(), 0, self.game_over\n",
        "    self.board[move[0]][move[1]] = self.players.index(self.current_player) + 1\n",
        "    self.check_winner()\n",
        "    reward = 1 if self.winner == \"X\" else -1 if self.winner == \"O\" else 0\n",
        "    self.switch_player()\n",
        "    return self.board.copy(), reward, self.game_over\n",
        "\n",
        "  # Switches Players\n",
        "  def switch_player(self):\n",
        "    if self.current_player == self.players[0]:\n",
        "      self.current_player = self.players[1]\n",
        "    else:\n",
        "      self.current_player = self.players[0]\n",
        "\n",
        "  # Checks for a Winner\n",
        "  def check_winner(self):\n",
        "    lines = []\n",
        "\n",
        "    lines.extend(self.board)\n",
        "    lines.extend(self.board.T)\n",
        "    lines.append(np.diag(self.board))\n",
        "    lines.append(np.diag(np.fliplr(self.board)))\n",
        "\n",
        "    for line in lines:\n",
        "      if np.all(line == 1):\n",
        "        self.winner = \"X\"\n",
        "        self.game_over = True\n",
        "        return\n",
        "\n",
        "      if np.all(line == 2):\n",
        "        self.winner = \"O\"\n",
        "        self.game_over = True\n",
        "        return\n",
        "\n",
        "    if not np.any(self.board == 0):\n",
        "      self.game_over = True\n",
        "\n",
        "  # Print the Current State of the Board\n",
        "  def print_board(self):\n",
        "    print(\"-\"*(self.height * 4 + 1))\n",
        "    for i in range(self.height):\n",
        "      print(\"|\", end=\" \")\n",
        "      for j in range(self.width):\n",
        "        print(self.players[int(self.board[i][j] - 1)] if self.board[i][j] != 0 else \" \", end=\" | \")\n",
        "      print()\n",
        "      print(\"-\"*(self.height * 4 + 1))"
      ],
      "metadata": {
        "id": "LyyHU-hDOCi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the TicTacToe Object with Playing the Game\n",
        "game = TicTacToe(3, 3)\n",
        "game.current_player = game.players[0]\n",
        "game.print_board()\n",
        "\n",
        "while not game.game_over:\n",
        "  move = input(f\"{game.current_player}'s turn. Enter row and column (e.g. 0 0): \")\n",
        "  move = tuple(map(int, move.split()))\n",
        "  while move not in game.available_moves():\n",
        "    move = input(\"Invalid move. Try again: \")\n",
        "    move = tuple(map(int, move.split()))\n",
        "  game.make_move(move)\n",
        "  game.print_board()\n",
        "\n",
        "if game.winner:\n",
        "  print(f\"{game.winner} wins!\")\n",
        "else:\n",
        "  print(\"It's a tie!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctJdg4nWTAfw",
        "outputId": "5da81b2f-3e13-4597-91d7-b602c10b97cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "X's turn. Enter row and column (e.g. 0 0): 1 1\n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "O's turn. Enter row and column (e.g. 0 0): 0 0\n",
            "-------------\n",
            "| O |   |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "X's turn. Enter row and column (e.g. 0 0): 1 0\n",
            "-------------\n",
            "| O |   |   | \n",
            "-------------\n",
            "| X | X |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "O's turn. Enter row and column (e.g. 0 0): 0 1\n",
            "-------------\n",
            "| O | O |   | \n",
            "-------------\n",
            "| X | X |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "X's turn. Enter row and column (e.g. 0 0): 1 2\n",
            "-------------\n",
            "| O | O |   | \n",
            "-------------\n",
            "| X | X | X | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "X wins!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q-learning Agent**"
      ],
      "metadata": {
        "id": "Rzfqk8vSUuof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "  def __init__(self, alpha, epsilon, discount_factor):\n",
        "    self.Q = {}\n",
        "    self.alpha = alpha\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma = discount_factor\n",
        "\n",
        "  # Method retrieves the Q-value for a specific state-action pair\n",
        "  def get_Q_value(self, state_key, action):\n",
        "    if (state_key, action) not in self.Q:\n",
        "        self.Q[(state_key, action)] = 0.0\n",
        "    return self.Q[(state_key, action)]\n",
        "\n",
        "  # Method selects an action based on the current state and available moves\n",
        "  def choose_action(self, state_key, available_moves):\n",
        "    if random.random() < self.epsilon:\n",
        "        return random.choice(available_moves)\n",
        "\n",
        "    Q_values = [self.get_Q_value(state_key, a) for a in available_moves]\n",
        "    max_Q = max(Q_values)\n",
        "    best_moves = [a for a, q in zip(available_moves, Q_values) if q == max_Q]\n",
        "    return random.choice(best_moves)\n",
        "\n",
        "  # Updates the Q-value using Bellman Equation\n",
        "  def update_Q_value(self, state_key, action, reward, next_state_key, next_available_moves):\n",
        "    max_next_Q = max([self.get_Q_value(next_state_key, a) for a in next_available_moves], default=0)\n",
        "    old_Q = self.get_Q_value(state_key, action)\n",
        "    self.Q[(state_key, action)] = old_Q + self.alpha * (reward + self.gamma * max_next_Q - old_Q)"
      ],
      "metadata": {
        "id": "5TGL-ey0Uxsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "znfog-xBbD0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_episodes, alpha, epsilon, discount_factor):\n",
        "  agent = QLearningAgent(alpha, epsilon, discount_factor)\n",
        "\n",
        "  # Loop through the specified number of training episodes\n",
        "  for episode in range(num_episodes):\n",
        "    env = TicTacToe(3, 3)\n",
        "    state = env.board.copy()\n",
        "    env.current_player = env.players[0]\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      # Convert state to tuple for dict key\n",
        "      state_key = tuple(state.flatten()) + (env.current_player,)\n",
        "\n",
        "      available_moves = env.available_moves()\n",
        "      action = agent.choose_action(state_key, available_moves)\n",
        "\n",
        "      # Agent makes move\n",
        "      next_state, reward, done = env.make_move(action)\n",
        "      next_available_moves = env.available_moves()\n",
        "\n",
        "      # If game not over, opponent makes a random move\n",
        "      if not done and next_available_moves:\n",
        "        env.current_player = env.players[1]\n",
        "        opp_action = random.choice(next_available_moves)\n",
        "        next_state, reward, done = env.make_move(opp_action)\n",
        "\n",
        "      next_state_key = tuple(next_state.flatten()) + (env.current_player,)\n",
        "      next_available_moves = env.available_moves()\n",
        "\n",
        "      # Update Q-table\n",
        "      agent.update_Q_value(state_key, action, reward, next_state_key, next_available_moves)\n",
        "\n",
        "      state = next_state.copy()\n",
        "\n",
        "  return agent\n"
      ],
      "metadata": {
        "id": "_6j9116HVzCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "TQaPJCLKdSRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(agent, num_games):\n",
        "  num_wins = 0\n",
        "\n",
        "  # Loop through number of games\n",
        "  for _ in range(num_games):\n",
        "\n",
        "    # Make a new game environment\n",
        "    env = TicTacToe(3, 3)\n",
        "    state = env.board.copy()\n",
        "    env.current_player = env.players[0]\n",
        "    done = False\n",
        "    reward = 0\n",
        "\n",
        "    # Loop through all the action needed before the game ends\n",
        "    while not done:\n",
        "      state_key = tuple(state.flatten()) + (env.current_player,)\n",
        "      available_moves = env.available_moves()\n",
        "\n",
        "      if env.current_player == env.players[0]:\n",
        "          # Agent's turn\n",
        "          action = agent.choose_action(state_key, available_moves)\n",
        "      else:\n",
        "          # Opponent's turn\n",
        "          action = random.choice(available_moves)\n",
        "\n",
        "      state, reward, done = env.make_move(action)\n",
        "\n",
        "    # Add the number of wins the Agent gets\n",
        "    if reward == 1:\n",
        "      num_wins += 1\n",
        "\n",
        "  return num_wins / num_games * 100\n"
      ],
      "metadata": {
        "id": "I5UGimY_dTaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run**"
      ],
      "metadata": {
        "id": "CZrPXqNMend-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Agent\n",
        "agent = train(num_episodes=100000, alpha=0.5, epsilon=0.1, discount_factor=1.0)\n",
        "\n",
        "# Test the Agent and Get the Win Percentage\n",
        "win_percentage = test(agent, num_games=1000)\n",
        "print(\"Win percentage: {:.2f}%\".format(win_percentage))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_16BVjlepVd",
        "outputId": "7382ba04-3b51-47fa-cd17-ec2734b12eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Win percentage: 92.10%\n"
          ]
        }
      ]
    }
  ]
}